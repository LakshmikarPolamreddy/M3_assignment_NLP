{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/Natural-Language-Processing-YU/Module-5-Assignment/blob/main/scripts/Part%20I.ipynb","timestamp":1689611346818}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KBtJv0BsJxNx"},"source":["##Part I:  Vector Semantics and Motivation for Word Embeddings\n","\n","It is important to understand the words meaning (recall semantics) AND their context. Words that are seen in the similar context also often have similar meaning. The distributional hypothesis expresses this phenomenon saying that there is a link in similarity in how words are distributed and their likeness.  Vector semantics is the concept of learning representations of meanings of words – called embeddings—from their distributions in a corpus or corpora. Fundamentally, we are asking the question with NLP: how might we represent the meaning of a word and interpret it?\n","\n","A word embedding is simply a to represent words in a numerical context -- a vector.  This is important because Neural Networks and Machine Learning models don't learn on the text itself, but the numerical representation of the text. In fact, there is typically an \"embedding layer\" as part of the simplest NLP-based neural networks as you will find.\n","\n","The simplest way to show this is called a one-hot vector, other forms include term frequencies of words (as we have seen with Bayesian models), Term Frequency-Inverse Document Frequency, which normalizes terms across documents, and distributional representations, which are context-based encodings that help derive similarity-- i.e., \"queen is to female as king is to male\".\n","\n","We will start simple and discuss some of the challenges, then move to more complex transformations.\n","\n","## Setup\n","As part of completing the assignment, you will see that there are areas in the note book for you to complete your own coding input.\n","\n","It will be look like following:\n","```\n","### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n","'Some coding activity for you to complete'\n","### END CODE HERE ###\n","\n","```\n","Please be sure to fill these code snippets out as you turn in your assignment.\n"]},{"cell_type":"markdown","metadata":{"id":"mKPWU-Ip2AZL"},"source":["##1.1 One-hot vector\n","\n","A one-hot vector helps to translate categorical or sequential data to something that is machine readable and also does not have an impact on your model. Each word in the sequence is given a binary encoding and is mapped to a vector of the length of the the input. This is a common pre-processing step for the input layer in a neural network.\n","\n","One hot encoding assigns a unique code for each unique word. As an example, we can take the following sentence and convert it to a one-hot vector.\n","\n","\"Live as if you were to die tomorrow. Learn as if you were to live forever\"\n","\n","We will use NLTK to tokenize the sentence, then Sci-Kit Learn to apply the one-hot encoder. Note, that SK-Learn will apply a single value for a unique word in a vector which is great for categorical representations. This one-hot encoding has traditionally been used for feeding categorical data to many scikit-learn estimators in shallow learning models such as notably linear models and SVMs with the standard kernels.\n","\n","Note: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":692},"id":"V5pWPsfD2BfJ","outputId":"f5b00275-7ed9-4474-c968-627364b7064d","executionInfo":{"status":"ok","timestamp":1689811393497,"user_tz":240,"elapsed":3651,"user":{"displayName":"Lakshmikar Reddy Polamreddy","userId":"01522637219110771790"}}},"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","import re\n","import string\n","from numpy import argmax\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import RegexpTokenizer\n","from sklearn.linear_model import LinearRegression\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder\n","nltk.download('punkt')\n","\n","#%matplotlib inline\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","# define input string\n","data = 'Live as if you were to die tomorrow. Learn as if you were to live forever'\n","#tokenize that string\n","wordlist = nltk.word_tokenize(data.lower())\n","#create a vector representation of the wordlist\n","wordlist_clean = []\n","\n","for i in wordlist: # Go through every word in your tokens list\n","    if (i not in string.punctuation):  # remove punctuation\n","        wordlist_clean.append(i)\n","# define universe of possible input values\n","wordlist_clean_df = pd.DataFrame(data=wordlist_clean, columns=['words'])\n","\n","#encode using scki-kit learn\n","one_hot_encoder = OneHotEncoder(sparse=False)\n","one_hot_encoder.fit(wordlist_clean_df)\n","wordlist_clean_df_encoded = one_hot_encoder.transform(wordlist_clean_df)\n","wordlist_clean_df_encoded = pd.DataFrame(data=wordlist_clean_df_encoded, columns=one_hot_encoder.categories_)\n","print('\\n\\n One-Hot Encoded Vector using SKLearn')\n","display(wordlist_clean_df_encoded)"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n"," One-Hot Encoded Vector using SKLearn\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["     as  die forever   if learn live   to tomorrow were  you\n","0   0.0  0.0     0.0  0.0   0.0  1.0  0.0      0.0  0.0  0.0\n","1   1.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n","2   0.0  0.0     0.0  1.0   0.0  0.0  0.0      0.0  0.0  0.0\n","3   0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  1.0\n","4   0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  1.0  0.0\n","5   0.0  0.0     0.0  0.0   0.0  0.0  1.0      0.0  0.0  0.0\n","6   0.0  1.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n","7   0.0  0.0     0.0  0.0   0.0  0.0  0.0      1.0  0.0  0.0\n","8   0.0  0.0     0.0  0.0   1.0  0.0  0.0      0.0  0.0  0.0\n","9   1.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n","10  0.0  0.0     0.0  1.0   0.0  0.0  0.0      0.0  0.0  0.0\n","11  0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  1.0\n","12  0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  1.0  0.0\n","13  0.0  0.0     0.0  0.0   0.0  0.0  1.0      0.0  0.0  0.0\n","14  0.0  0.0     0.0  0.0   0.0  1.0  0.0      0.0  0.0  0.0\n","15  0.0  0.0     1.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0"],"text/html":["\n","\n","  <div id=\"df-385f8bbd-09b9-47b6-832c-caa5597f4fb7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th>as</th>\n","      <th>die</th>\n","      <th>forever</th>\n","      <th>if</th>\n","      <th>learn</th>\n","      <th>live</th>\n","      <th>to</th>\n","      <th>tomorrow</th>\n","      <th>were</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-385f8bbd-09b9-47b6-832c-caa5597f4fb7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-de575a59-6621-48c1-89df-8e696b82766a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de575a59-6621-48c1-89df-8e696b82766a')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-de575a59-6621-48c1-89df-8e696b82766a button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-385f8bbd-09b9-47b6-832c-caa5597f4fb7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-385f8bbd-09b9-47b6-832c-caa5597f4fb7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"aTRMr2m-oxRO"},"source":["##1.2 Encoding as a dense  - Singular Value Decomposition\n","A second approach you might try is to encode each word using a unique number. This helps with reducing dimensionality and attempts to address the problem of very large sparse matrices. Continuing the example above, you could assign 1 to \"live\", 2 to \"the\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector. Now, instead of a sparse vector, you now have a dense one. A dense vector is a vector where all elements are populated with a non-zero value.\n","\n","There are several challenges:\n","\n","1.   The integer-encoding is arbitrary (it does not capture any relationship between words)\n","2.   An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n","3.  Word order is ignored.\n","4.  Raw absolute frequency counts of words do not necessarily represent the meaning of the text properly\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"id":"0C-2Kg9Dq05v","outputId":"5d672439-de1c-43d7-e5c2-61b924220b65","executionInfo":{"status":"ok","timestamp":1689611413684,"user_tz":240,"elapsed":209,"user":{"displayName":"Lakshmikar Reddy Polamreddy","userId":"01522637219110771790"}}},"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","import re\n","import string\n","from numpy import argmax\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import RegexpTokenizer\n","nltk.download('punkt')\n","\"\"\"## Default Style Settings\n","matplotlib.rcParams['figure.dpi'] = 150\n","pd.options.display.max_colwidth = 200\n","#%matplotlib inline\"\"\"\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","# define input string\n","data = 'Live as if you were to die tomorrow. Learn as if you were to live forever'\n","#tokenize that string\n","wordlist = nltk.word_tokenize(data.lower())\n","#create a vector representation of the wordlist\n","wordlist_clean = []\n","\n","for i in wordlist: # Go through every word in your tokens list\n","    if (i not in string.punctuation):  # remove punctuation\n","        wordlist_clean.append(i)\n","# define universe of possible input values\n","wordlist_clean_df = pd.DataFrame(data=wordlist_clean, columns=['words'])\n","dense_vector = np.unique(wordlist_clean_df, return_counts=True)\n","dense_vector_df = pd.DataFrame(data=dense_vector, columns = np.unique(wordlist_clean_df))\n","display(dense_vector_df)\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"display_data","data":{"text/plain":["   as  die  forever  if  learn  live  to  tomorrow  were  you\n","0  as  die  forever  if  learn  live  to  tomorrow  were  you\n","1   2    1        1   2      1     2   2         1     2    2"],"text/html":["\n","\n","  <div id=\"df-f6c76fba-ff07-4046-94d1-6df9b69ccf0d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>as</th>\n","      <th>die</th>\n","      <th>forever</th>\n","      <th>if</th>\n","      <th>learn</th>\n","      <th>live</th>\n","      <th>to</th>\n","      <th>tomorrow</th>\n","      <th>were</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>as</td>\n","      <td>die</td>\n","      <td>forever</td>\n","      <td>if</td>\n","      <td>learn</td>\n","      <td>live</td>\n","      <td>to</td>\n","      <td>tomorrow</td>\n","      <td>were</td>\n","      <td>you</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f6c76fba-ff07-4046-94d1-6df9b69ccf0d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-060c6c9c-0ae8-4800-acb2-7eda88f644cd\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-060c6c9c-0ae8-4800-acb2-7eda88f644cd')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-060c6c9c-0ae8-4800-acb2-7eda88f644cd button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f6c76fba-ff07-4046-94d1-6df9b69ccf0d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f6c76fba-ff07-4046-94d1-6df9b69ccf0d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"X5217etS2QW5"},"source":["##1.3 Text Vectorization\n","\n","\n","*   Overview\n","*   N-Gram Bag of Words\n","*   Term Frequency-Inverse Document Frequency (TF-IDF)\n","*   Document Similarity: Cosine Similarity, Jaccard Similarity, Euclidian Similarity\n","*   Topic Modeling Exercise\n","\n","### Why do we do it?\n","These subsquent categories of text vectorizations are ways to derive similarity from text documents. This is useful for NLP tasks such as topic modeling -- where we aim to show the relationship between documents via a category or topic. You will see how TF-IDF can be used to support topic modeling.\n","\n","Here are some text vectorization approaches in summary:\n","![Text Vectorization Approaches](https://drive.google.com/uc?export=view&id=12GYWDaK5_offSn3Gy-hv_KpuTc4A_mGA)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vKq4TXKo1zvi"},"source":["\n","### 1.3.1 N-Gram Bag-Of Words Model\n","You've already learned the bag-of words model above with one-hot encoding and dense vectorization! We are counting the frequencies of words in the matrix in a dense representation of the word vector. What happens if we took some steps to improve the Bag-of-Words model by incorporating the n-gram approach we have learned earlier in the class.\n","\n","What does this do?\n","If our goal is to identify words in texts that represent meaning of that text, then recall that taking the bi-gram, tri-gram, or n-gram of a corpus allows us to bring in context via the word order. With a simple BOW approach, no word order is considers. Moreover, we can filter words based on distributional counts -- that is, term frequencies. Imagine that the counts of a word fall into say a Gaussian (normal) Distribution across a number of corpora. We can use the distribution to filter out salient word-phrases or sequences in which we can infer the meaning of the text. Finally, we can apply weights to the frequency counts -- similar to weight vector in a NN-- in which those weights have an impact on word relationships or salience.\n","\n","\n","![Bag of Words](https://drive.google.com/uc?export=view&id=1btCVz_8JWYTvE73qGLCRZb7nXg-kCiDU)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-gR43ePKDKlu"},"source":["#### 1.3.1.2 Example: N-Gram Bag-Of Words Model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":651},"id":"cv1H2Ha-0RMP","outputId":"6c4b7219-db6f-413a-cfd0-4ccb2a5732d2","executionInfo":{"status":"ok","timestamp":1689611740596,"user_tz":240,"elapsed":161,"user":{"displayName":"Lakshmikar Reddy Polamreddy","userId":"01522637219110771790"}}},"source":["#example from: https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/5-text-vectorization.html#\n","\n","import pandas as pd                        # Python library for pandas - data maniplation\n","import numpy as np                         # Python library for numpy -- matrix algebra library\n","import matplotlib                          # Python library for matplotlib -- visual display of data\n","import matplotlib.pyplot as plt            # Python library for matplotlib -- visual display of data\n","import nltk                                # Python library for NLP\n","import re                                  # library for regular expression operations\n","import string                              # for string operations\n","nltk.download('stopwords')                 # package for stop words\n","from nltk.corpus import stopwords          # module for stop words that come with NLTK\n","\n","from nltk.stem import PorterStemmer        # module for stemming\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","## Default Style Settings\n","matplotlib.rcParams['figure.dpi'] = 150\n","pd.options.display.max_colwidth = 200\n","#%matplotlib inline\n","\n","corpus = [\n","    'The sky is blue and beautiful.', 'Love this blue and beautiful sky!',\n","    'The quick brown fox jumps over the lazy dog.',\n","    \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n","    'I love green eggs, ham, sausages and bacon!',\n","    'The brown fox is quick and the blue dog is lazy!',\n","    'The sky is very blue and the sky is very beautiful today',\n","    'The dog is lazy but the brown fox is quick!'\n","]\n","labels = [\n","    'weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather',\n","    'animals'\n","]\n","\n","corpus = np.array(corpus) # np.array better than list\n","corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n","corpus_df\n","\n","wpt = nltk.WordPunctTokenizer()\n","stop_words = nltk.corpus.stopwords.words('english')\n","\n","def normalize_document(doc):\n","    # lower case and remove special characters\\whitespaces\n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n","    doc = doc.lower()\n","    doc = doc.strip()\n","    # tokenize document\n","    tokens = wpt.tokenize(doc)\n","    # filter stopwords out of document\n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    # re-create document from filtered tokens\n","    doc = ' '.join(filtered_tokens)\n","    return doc\n","\n","normalize_corpus = np.vectorize(normalize_document)\n","\n","norm_corpus = normalize_corpus(corpus)\n","print(corpus)\n","print(\"=\"*50)\n","print(norm_corpus)\n","\n","# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n","bv = CountVectorizer(ngram_range=(2, 2))\n","bv_matrix = bv.fit_transform(norm_corpus)\n","\n","bv_matrix = bv_matrix.toarray()\n","#vocab = bv.get_feature_names()\n","vocab = bv.get_feature_names_out()\n","pd.DataFrame(bv_matrix, columns=vocab)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['The sky is blue and beautiful.' 'Love this blue and beautiful sky!'\n"," 'The quick brown fox jumps over the lazy dog.'\n"," \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\"\n"," 'I love green eggs, ham, sausages and bacon!'\n"," 'The brown fox is quick and the blue dog is lazy!'\n"," 'The sky is very blue and the sky is very beautiful today'\n"," 'The dog is lazy but the brown fox is quick!']\n","==================================================\n","['sky blue beautiful' 'love blue beautiful sky'\n"," 'quick brown fox jumps lazy dog'\n"," 'kings breakfast sausages ham bacon eggs toast beans'\n"," 'love green eggs ham sausages bacon' 'brown fox quick blue dog lazy'\n"," 'sky blue sky beautiful today' 'dog lazy brown fox quick']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["   bacon eggs  beautiful sky  beautiful today  blue beautiful  blue dog  \\\n","0           0              0                0               1         0   \n","1           0              1                0               1         0   \n","2           0              0                0               0         0   \n","3           1              0                0               0         0   \n","4           0              0                0               0         0   \n","5           0              0                0               0         1   \n","6           0              0                1               0         0   \n","7           0              0                0               0         0   \n","\n","   blue sky  breakfast sausages  brown fox  dog lazy  eggs ham  ...  lazy dog  \\\n","0         0                   0          0         0         0  ...         0   \n","1         0                   0          0         0         0  ...         0   \n","2         0                   0          1         0         0  ...         1   \n","3         0                   1          0         0         0  ...         0   \n","4         0                   0          0         0         1  ...         0   \n","5         0                   0          1         1         0  ...         0   \n","6         1                   0          0         0         0  ...         0   \n","7         0                   0          1         1         0  ...         0   \n","\n","   love blue  love green  quick blue  quick brown  sausages bacon  \\\n","0          0           0           0            0               0   \n","1          1           0           0            0               0   \n","2          0           0           0            1               0   \n","3          0           0           0            0               0   \n","4          0           1           0            0               1   \n","5          0           0           1            0               0   \n","6          0           0           0            0               0   \n","7          0           0           0            0               0   \n","\n","   sausages ham  sky beautiful  sky blue  toast beans  \n","0             0              0         1            0  \n","1             0              0         0            0  \n","2             0              0         0            0  \n","3             1              0         0            1  \n","4             0              0         0            0  \n","5             0              0         0            0  \n","6             0              1         1            0  \n","7             0              0         0            0  \n","\n","[8 rows x 29 columns]"],"text/html":["\n","\n","  <div id=\"df-be412501-4caf-4a39-af2e-f014eb074b40\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bacon eggs</th>\n","      <th>beautiful sky</th>\n","      <th>beautiful today</th>\n","      <th>blue beautiful</th>\n","      <th>blue dog</th>\n","      <th>blue sky</th>\n","      <th>breakfast sausages</th>\n","      <th>brown fox</th>\n","      <th>dog lazy</th>\n","      <th>eggs ham</th>\n","      <th>...</th>\n","      <th>lazy dog</th>\n","      <th>love blue</th>\n","      <th>love green</th>\n","      <th>quick blue</th>\n","      <th>quick brown</th>\n","      <th>sausages bacon</th>\n","      <th>sausages ham</th>\n","      <th>sky beautiful</th>\n","      <th>sky blue</th>\n","      <th>toast beans</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>8 rows × 29 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be412501-4caf-4a39-af2e-f014eb074b40')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-0adac94e-6f6e-440d-b07f-ab5ec2b74431\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0adac94e-6f6e-440d-b07f-ab5ec2b74431')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-0adac94e-6f6e-440d-b07f-ab5ec2b74431 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-be412501-4caf-4a39-af2e-f014eb074b40 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-be412501-4caf-4a39-af2e-f014eb074b40');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"Be0ML9kADimc"},"source":["### 1.3.2 Term Frequency-Inverse Document Frequency (TF-IDF)\n","As an extension of the BOW model, we can weight the frequency (counts) of the terms in a document by considering its *dispersion*. Fundamentally, we are taken the total frequency of a word and dividing it by the number of documents with that term -- this gives us term frequency.\n","\n","\n","Then we take the inverse The formula for TF-IDF will look something like:\n","\n","\n","*   Term Frequency(TF): the number of times a word appears in a document. These are the raw absolute frequency counts of the words in the BOW model.\n","*   Inverse Document Frequency(IDF): total documents in corpus over number of documents with term.\n","\n","> $\\textit{TF-IDF} = {tf \\times idf}$\n","\n","Here, the general idea is that we can extropolate the meaningful words from a corpus by inversing their frequency. For example, \"The\" in the corpus may be frequently observed, but does not garner meaning. We can use this for keyword extraction, and information retrieval tasks.\n","\n","Let's normalize this function to account for divide-by-zero erros and to also smooth the weighting scheme.\n","\n","Addressing divide-by-zero errors. Similar to Laplace Smoothing techniques, we will typically add one to the IDF formula:\n","\n","> $\\textit{IDF} = 1 + log\\frac{N}{1+df}$\n","\n","We also might normalize the final IF-IDF function using an L2 Norm (see more in Jurafsky, Chapter 6).\n","\n","> $\\textit{TF-IDF}_{normalized} = \\frac{tf \\times idf}{\\sqrt{(tf\\times idf)^2}}$\n"]},{"cell_type":"markdown","metadata":{"id":"X4tJ1Yrbbufr"},"source":["#### 1.3.2.1 Example: TF-IDF Usage\n","In our example, we use the TfidfTransformer function to apply L2 norms and smoothing techniques.\n","\n","```\n","tt = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n","```\n","\n","\n","Let's use the same corpus from above in our example for TD-IDF"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":583},"id":"6KROXdPWWRlv","outputId":"b980ac46-8067-4881-a39a-2de14a4db888","executionInfo":{"status":"ok","timestamp":1689611870545,"user_tz":240,"elapsed":277,"user":{"displayName":"Lakshmikar Reddy Polamreddy","userId":"01522637219110771790"}}},"source":["norm_corpus = ['sky blue beautiful', 'love blue beautiful sky',\n"," 'quick brown fox jumps lazy dog',\n"," 'kings breakfast sausages ham bacon eggs toast beans',\n"," 'love green eggs ham sausages bacon', 'brown fox quick blue dog lazy',\n"," 'sky blue sky beautiful today' ,'dog lazy brown fox quick']\n","\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","# get bag of words features in sparse format\n","cv = CountVectorizer(min_df=0., max_df=1.)\n","cv_matrix = cv.fit_transform(norm_corpus)\n","cv_matrix\n","\n","from sklearn.feature_extraction.text import TfidfTransformer\n","\n","\n","\"\"\"Note: With Tfidftransformer you will systematically compute word counts using CountVectorizer\n","and then compute the Inverse Document Frequency (IDF) values and only then compute the Tf-idf scores.\"\"\"\n","tt = TfidfTransformer(norm='l2',\n","                      use_idf=True,\n","                      smooth_idf=True)\n","tt_matrix = tt.fit_transform(cv_matrix)\n","tt_matrix = tt_matrix.toarray()\n","#vocab = cv.get_feature_names()\n","vocab = cv.get_feature_names_out()\n","tt_df = pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)\n","display(tt_df)\n","\n","\n","\n","\"\"\"Note: WWith Tfidfvectorizer on the contrary, you will do all three steps at once.\n","Under the hood, it computes the word counts, IDF values, and Tf-idf scores all using the same dataset.\"\"\"\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tv = TfidfVectorizer(min_df=0.,\n","                     max_df=1.,\n","                     norm='l2',\n","                     use_idf=True,\n","                     smooth_idf=True)\n","tv_matrix = tv.fit_transform(norm_corpus)\n","tv_matrix = tv_matrix.toarray()\n","\n","vocab = tv.get_feature_names_out()\n","tv_df  = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n","display(tv_df)\n"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n","0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n","1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n","2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n","3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n","4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n","5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n","6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n","7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n","\n","    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n","0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n","1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n","2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n","3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n","4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n","5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n","6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n","7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "],"text/html":["\n","\n","  <div id=\"df-28d7de71-ec13-4e06-bab9-6fa7cfaab94e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bacon</th>\n","      <th>beans</th>\n","      <th>beautiful</th>\n","      <th>blue</th>\n","      <th>breakfast</th>\n","      <th>brown</th>\n","      <th>dog</th>\n","      <th>eggs</th>\n","      <th>fox</th>\n","      <th>green</th>\n","      <th>ham</th>\n","      <th>jumps</th>\n","      <th>kings</th>\n","      <th>lazy</th>\n","      <th>love</th>\n","      <th>quick</th>\n","      <th>sausages</th>\n","      <th>sky</th>\n","      <th>toast</th>\n","      <th>today</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.60</td>\n","      <td>0.53</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.60</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.49</td>\n","      <td>0.43</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.57</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.49</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.53</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.32</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.47</td>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.37</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.36</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.72</td>\n","      <td>0.00</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28d7de71-ec13-4e06-bab9-6fa7cfaab94e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-dac4b4e8-b63a-43ae-8c83-69b2b346c44e\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dac4b4e8-b63a-43ae-8c83-69b2b346c44e')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-dac4b4e8-b63a-43ae-8c83-69b2b346c44e button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-28d7de71-ec13-4e06-bab9-6fa7cfaab94e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-28d7de71-ec13-4e06-bab9-6fa7cfaab94e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n","0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n","1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n","2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n","3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n","4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n","5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n","6   0.00   0.00       0.36  0.32       0.00   0.00  0.00  0.00  0.00   0.00   \n","7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n","\n","    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n","0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00    0.0  \n","1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00    0.0  \n","2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00    0.0  \n","3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38    0.0  \n","4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00    0.0  \n","5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00    0.0  \n","6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.72   0.00    0.5  \n","7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00    0.0  "],"text/html":["\n","\n","  <div id=\"df-e941e34a-cc93-40c5-9fe7-538268737c38\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bacon</th>\n","      <th>beans</th>\n","      <th>beautiful</th>\n","      <th>blue</th>\n","      <th>breakfast</th>\n","      <th>brown</th>\n","      <th>dog</th>\n","      <th>eggs</th>\n","      <th>fox</th>\n","      <th>green</th>\n","      <th>ham</th>\n","      <th>jumps</th>\n","      <th>kings</th>\n","      <th>lazy</th>\n","      <th>love</th>\n","      <th>quick</th>\n","      <th>sausages</th>\n","      <th>sky</th>\n","      <th>toast</th>\n","      <th>today</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.60</td>\n","      <td>0.53</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.60</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.49</td>\n","      <td>0.43</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.57</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.49</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.53</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.32</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.47</td>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.39</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.37</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.42</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.36</td>\n","      <td>0.32</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.72</td>\n","      <td>0.00</td>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.45</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e941e34a-cc93-40c5-9fe7-538268737c38')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-a00600a0-6765-4619-9236-dee6003b829f\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a00600a0-6765-4619-9236-dee6003b829f')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-a00600a0-6765-4619-9236-dee6003b829f button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e941e34a-cc93-40c5-9fe7-538268737c38 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e941e34a-cc93-40c5-9fe7-538268737c38');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"xfXbJjDdV5V4"},"source":["### 1.3.4 Document Similarity and Word Semantics\n","\n","Lexical semantics is a branch of linguistics focused on meaning and word relationships. Moreover, the idea behind word sense is the interpretation of the word (often requiring context to understand). Where multiple meanings can occur for a word – take the example of a mouse that can mean both the cursor controller and the rodent—we must discern using context. Relationships between word senses can be referred to as synonyms (e.g., couch/sofa).\n","\n","**Word Similarity** is not the same as a synonym, rather it is the idea that words have relationships. The example of cat and dog is used to show that while they are not synonymous, they are both animals, often they are domesticated – their semantics are similar.\n","\n","**Word relatedness** is slightly different than word similarity where there is more of a psychological association—for example, that coffee and cup are related.\n","Recall that vectors for representing words are called embeddings implying that a point in space can be mapped to another point in space.\n","\n","This is important because word similarity (measured through a vector representing distance between two words in space) can be powerful for tasks we have previously done, such as sentiment analysis. Moreover, we can derive the meaning of the word using the nearby counts of similar words.\n","\n","We look at three similarity metrics to score word and/or document similarity:\n","\n","*   Manhattan Distance: is the sum of absolute differences between points across all the dimensions. Called \"Manhattan\" because we can think of getting from point (a,b) to point (c,d) on a Cartesian plane by only travelining vertically or horizontally, not diagnally.\n","*   Euclidian Distance: is the shortest distance between two points in mathmatics. Not as useful in the field of NLP. The \"as the crow flies\" distance.\n","*   Cosine Similarity: measure similarity based on the content overlap between documents.\n","*   Jaccard Similarity: Used to identify documents we measure it as proportion of number of common words to number of unique words in both documents.\n","\n","Note: Generally speaking the difference betweem *distance* and *similarity* is basically that distance is just equal to 1 - similarity.\n","\n","\n","Let's take a look at Cosine Similarity metrics since this is most commonly used with NLP and also with word2vec.\n","\n","> $similarity(doc_1, doc_2) = cos(\\theta) = \\frac{doc_1  doc_2}{\\lvert doc_1\\rvert \\lvert doc_2\\rvert}$\n","\n","By cosine distance/dissimilarity we assume following:\n","> $distance(doc_1, doc_2) = 1 - similarity(doc_1, doc_2)$\n","\n","The similarity-based metics look like the following🇰\n","> cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\n","\n","\n","\n","```\n","cosine_similarity(xyz)\n","array([[1.        , 0.97780241, 0.30320366],\n","       [0.97780241, 1.        , 0.49613894],\n","       [0.30320366, 0.49613894, 1.        ]])\n","```\n","\n","![](https://drive.google.com/uc?export=view&id=1c9D33toCdC1W3_SRiUawykcspho8IVfI)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aBFAAHro4KyS"},"source":["### **1.3.5: Exercise: BOW with n-gram**\n","Use the *brown* corpus to create a n-gram BOW model. First, you must clean and organize the data. Then enter your code to complete the exercise.\n","\n","The Brown Corpus is an collection of text samples of American English categorized by various genres such as science-fiction, adventure, etc.\n","\n","Create a tri-gram bag of words matrix using the brown corpus as its inputs.\n","\n"]},{"cell_type":"code","metadata":{"id":"wrrT-i-u4_7M","colab":{"base_uri":"https://localhost:8080/","height":904},"executionInfo":{"status":"ok","timestamp":1689613649920,"user_tz":240,"elapsed":3893,"user":{"displayName":"Lakshmikar Reddy Polamreddy","userId":"01522637219110771790"}},"outputId":"6a8b164d-a87c-4b74-a0ed-fceffb9dadba"},"source":["import pandas as pd                        # Python library for pandas - data maniplation\n","import numpy as np                         # Python library for numpy -- matrix algebra library\n","import matplotlib                          # Python library for matplotlib -- visual display of data\n","import matplotlib.pyplot as plt            # Python library for matplotlib -- visual display of data\n","import nltk                                # Python library for NLP\n","import re                                  # library for regular expression operations\n","import string                              # for string operations\n","\n","nltk.download('stopwords')                 # package for stop words\n","nltk.download('brown')                 # package for stop words\n","from nltk.corpus import stopwords          # module for stop words that come with NLTK\n","from nltk.corpus import brown              # this is the corpus you use for this exercise.\n","from nltk.stem import PorterStemmer        # module for stemming\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","#The seed() method is used to initialize the random number generator\n","np.random.seed(100)\n","\n","brown_cat= brown.categories() # Creates a list of categories\n","\n","docs=[]\n","for cat in brown_cat: # We append tuples of each document and categories in a list\n","    t1=brown.sents(categories=cat) # At each iteration we retrieve all documents of a given category\n","    for doc in t1:\n","        docs.append((' '.join(doc), cat)) # These documents are appended as a tuple (document, category) in the list\n","\n","brown_df=pd.DataFrame(docs, columns=['sentence', 'category']) #The data frame is created using the generated tuple.\n","\n","brown_df.head()\n","\n","\n","#Step 1. Pre-Processing the Brown Corpus Text\n","wpt = nltk.WordPunctTokenizer()\n","stop_words = nltk.corpus.stopwords.words('english')\n","\n","def normalize_document(doc):\n","    # lower case and remove special characters\\whitespaces\n","    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n","    doc = doc.lower()\n","    doc = doc.strip()\n","    # tokenize document\n","    tokens = wpt.tokenize(doc)\n","    # filter stopwords out of document\n","    filtered_tokens = [token for token in tokens if token not in stop_words]\n","    # re-create document from filtered tokens\n","    doc = ' '.join(filtered_tokens)\n","    return doc\n","\n","normalize_corpus = np.vectorize(normalize_document)\n","\n","### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n","\n","#create some normalized corpus from the pre-processing functiong above\n","brown_corpus = brown_df.sample(n=10000, random_state=100)\n","corpus = brown_corpus['sentence']\n","norm_corpus = normalize_corpus(corpus)\n","print(corpus)\n","print(\"=\"*50)\n","print(norm_corpus)\n","#Using the nromalized corpus.\n","#Because the brown corpus is very large,select 10,000 random records from the corpus. Set seed so you can return the same results.\n","\n","#Step 2. Create a tri-gram data frame and count its frequencies\n","# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n","tv = CountVectorizer(ngram_range=(3, 3))\n","tv_matrix = tv.fit_transform(norm_corpus)\n","\n","tv_matrix = tv_matrix.toarray()\n","vocab = tv.get_feature_names_out()\n","\n","\n","#print the dataframe to show the tri-gram BOW\n","pd.DataFrame(tv_matrix, columns=vocab)\n","\n","### END CODE HERE ###"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["52700    His lawyer had sent him a statement on his overdue alimony , and there was a letter from the Collector of Internal Revenue asking him to stop in his office and explain last year's exemptions .\n","19438                                                                                                        In this role of father confessor , he has always been most characteristic and most helpful .\n","56419                                                                                                                                                    Mike felt disappointed that Mahmoud had left ; ;\n","32385                                                                                                                       and this he could do only by going over its mass with the tracing procedure .\n","16328                                                                                                                                            John recognized Ablard Corne and called out a greeting .\n","                                                                                                       ...                                                                                               \n","8121                                                                                                                                                                       Gross swung his swivel chair .\n","29194                                                                                                                                           For the marksman , we study sets of five shots ( Af ) ; ;\n","43239                                                                                                                                                                            No one saw Stacy leave .\n","13651                                                                                                        This is our duty -- not as nurses or city employes -- but as citizens of the United States .\n","47336                                                                                                                                `` The nuns there do a wonderful work '' , the President commented .\n","Name: sentence, Length: 10000, dtype: object\n","==================================================\n","['lawyer sent statement overdue alimony letter collector internal revenue asking stop office explain last years exemptions'\n"," 'role father confessor always characteristic helpful'\n"," 'mike felt disappointed mahmoud left' ... 'one saw stacy leave'\n"," 'duty nurses city employes citizens united states'\n"," 'nuns wonderful work president commented']\n"]},{"output_type":"execute_result","data":{"text/plain":["      aab follows vowel  aah go said  aaron cohn san  ab plane end  \\\n","0                     0            0               0             0   \n","1                     0            0               0             0   \n","2                     0            0               0             0   \n","3                     0            0               0             0   \n","4                     0            0               0             0   \n","...                 ...          ...             ...           ...   \n","9995                  0            0               0             0   \n","9996                  0            0               0             0   \n","9997                  0            0               0             0   \n","9998                  0            0               0             0   \n","9999                  0            0               0             0   \n","\n","      ab plasma control  abandon atom bomb  abandon ceremonies winter  \\\n","0                     0                  0                          0   \n","1                     0                  0                          0   \n","2                     0                  0                          0   \n","3                     0                  0                          0   \n","4                     0                  0                          0   \n","...                 ...                ...                        ...   \n","9995                  0                  0                          0   \n","9996                  0                  0                          0   \n","9997                  0                  0                          0   \n","9998                  0                  0                          0   \n","9999                  0                  0                          0   \n","\n","      abandon efforts would  abandon real celebration  abandon whole life  \\\n","0                         0                         0                   0   \n","1                         0                         0                   0   \n","2                         0                         0                   0   \n","3                         0                         0                   0   \n","4                         0                         0                   0   \n","...                     ...                       ...                 ...   \n","9995                      0                         0                   0   \n","9996                      0                         0                   0   \n","9997                      0                         0                   0   \n","9998                      0                         0                   0   \n","9999                      0                         0                   0   \n","\n","      ...  zion stayed get  zodiacal sign watercolorist  zoning board county  \\\n","0     ...                0                            0                    0   \n","1     ...                0                            0                    0   \n","2     ...                0                            0                    0   \n","3     ...                0                            0                    0   \n","4     ...                0                            0                    0   \n","...   ...              ...                          ...                  ...   \n","9995  ...                0                            0                    0   \n","9996  ...                0                            0                    0   \n","9997  ...                0                            0                    0   \n","9998  ...                0                            0                    0   \n","9999  ...                0                            0                    0   \n","\n","      zoo grimace lions  zoo reservations extremely  zoooop snag around  \\\n","0                     0                           0                   0   \n","1                     0                           0                   0   \n","2                     0                           0                   0   \n","3                     0                           0                   0   \n","4                     0                           0                   0   \n","...                 ...                         ...                 ...   \n","9995                  0                           0                   0   \n","9996                  0                           0                   0   \n","9997                  0                           0                   0   \n","9998                  0                           0                   0   \n","9999                  0                           0                   0   \n","\n","      zu longing simple  zur khaneh latter  zurich prince boun  \\\n","0                     0                  0                   0   \n","1                     0                  0                   0   \n","2                     0                  0                   0   \n","3                     0                  0                   0   \n","4                     0                  0                   0   \n","...                 ...                ...                 ...   \n","9995                  0                  0                   0   \n","9996                  0                  0                   0   \n","9997                  0                  0                   0   \n","9998                  0                  0                   0   \n","9999                  0                  0                   0   \n","\n","      zworykin novel technique  \n","0                            0  \n","1                            0  \n","2                            0  \n","3                            0  \n","4                            0  \n","...                        ...  \n","9995                         0  \n","9996                         0  \n","9997                         0  \n","9998                         0  \n","9999                         0  \n","\n","[10000 rows x 72736 columns]"],"text/html":["\n","\n","  <div id=\"df-6655dfb1-ef63-4ed8-b3bf-7e4f2ab2de9e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>aab follows vowel</th>\n","      <th>aah go said</th>\n","      <th>aaron cohn san</th>\n","      <th>ab plane end</th>\n","      <th>ab plasma control</th>\n","      <th>abandon atom bomb</th>\n","      <th>abandon ceremonies winter</th>\n","      <th>abandon efforts would</th>\n","      <th>abandon real celebration</th>\n","      <th>abandon whole life</th>\n","      <th>...</th>\n","      <th>zion stayed get</th>\n","      <th>zodiacal sign watercolorist</th>\n","      <th>zoning board county</th>\n","      <th>zoo grimace lions</th>\n","      <th>zoo reservations extremely</th>\n","      <th>zoooop snag around</th>\n","      <th>zu longing simple</th>\n","      <th>zur khaneh latter</th>\n","      <th>zurich prince boun</th>\n","      <th>zworykin novel technique</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 72736 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6655dfb1-ef63-4ed8-b3bf-7e4f2ab2de9e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-3b95ff98-c171-4c8a-97c8-718fbce1fe09\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b95ff98-c171-4c8a-97c8-718fbce1fe09')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-3b95ff98-c171-4c8a-97c8-718fbce1fe09 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6655dfb1-ef63-4ed8-b3bf-7e4f2ab2de9e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6655dfb1-ef63-4ed8-b3bf-7e4f2ab2de9e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"m1I-GlKzVk7B"},"source":["### 1.3.6 Exercise: TD-IDF\n","\n","Now, using anyone of the following datasets, create you're own TF-IDF implementation. Provide your output in the form of a matrix.\n","\n","For more on the intuition behind TF-IDF, read the article [here](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html).\n","\n","Refer to [this article](https://sci2lab.github.io/ml_tutorial/tfidf/) related to TF-IDF and Elastisearch. Note how the TF-IDF approach can be used for information retrieval.\n","\n","Datasets that you may choose from:\n","*   [Reviews Dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). this dataset uses classified data from Yelp!, Amazon, and IMBD. You can use this to determine TF-IDF across the datasets.\n","\n","\n","*   Presidential speeches in NLTK. You can use this dataset to determine the TFIDF vector of words across presidential speeches.\n","\n","```\n","nltk.corpus.inaugural\n","```\n","\n","Please provide your code in the cell below.\n","\n","\n","\n"]},{"cell_type":"code","source":["import nltk\n","import pandas as pd\n","from nltk.corpus import inaugural\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Step 1: Download the inaugural dataset from NLTK\n","nltk.download('inaugural')\n","\n","# Step 2: Load the dataset and preprocess the text data\n","def load_inaugural_speeches():\n","    speeches = [inaugural.raw(fileid) for fileid in inaugural.fileids()]\n","    return speeches\n","\n","speeches = load_inaugural_speeches()\n","\n","print(speeches[0])\n","\n","tv = TfidfVectorizer(min_df=0.,\n","                     max_df=1.,\n","                     norm='l2',\n","                     use_idf=True,\n","                     smooth_idf=True)\n","tv_matrix = tv.fit_transform(speeches)\n","tv_matrix = tv_matrix.toarray()\n","\n","vocab = tv.get_feature_names_out()\n","tv_df  = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n","display(tv_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4FRzXRSX3MP-","executionInfo":{"status":"ok","timestamp":1689615010365,"user_tz":240,"elapsed":373,"user":{"displayName":"Lakshmikar Reddy Polamreddy","userId":"01522637219110771790"}},"outputId":"47622354-7b86-4bd9-e1f8-158364bc240c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Fellow-Citizens of the Senate and of the House of Representatives:\n","\n","Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable decision, as the asylum of my declining years -- a retreat which was rendered every day more necessary as well as more dear to me by the addition of habit to inclination, and of frequent interruptions in my health to the gradual waste committed on it by time. On the other hand, the magnitude and difficulty of the trust to which the voice of my country called me, being sufficient to awaken in the wisest and most experienced of her citizens a distrustful scrutiny into his qualifications, could not but overwhelm with despondence one who (inheriting inferior endowments from nature and unpracticed in the duties of civil administration) ought to be peculiarly conscious of his own deficiencies. In this conflict of emotions all I dare aver is that it has been my faithful study to collect my duty from a just appreciation of every circumstance by which it might be affected. All I dare hope is that if, in executing this task, I have been too much swayed by a grateful remembrance of former instances, or by an affectionate sensibility to this transcendent proof of the confidence of my fellow citizens, and have thence too little consulted my incapacity as well as disinclination for the weighty and untried cares before me, my error will be palliated by the motives which mislead me, and its consequences be judged by my country with some share of the partiality in which they originated.\n","\n","Such being the impressions under which I have, in obedience to the public summons, repaired to the present station, it would be peculiarly improper to omit in this first official act my fervent supplications to that Almighty Being who rules over the universe, who presides in the councils of nations, and whose providential aids can supply every human defect, that His benediction may consecrate to the liberties and happiness of the people of the United States a Government instituted by themselves for these essential purposes, and may enable every instrument employed in its administration to execute with success the functions allotted to his charge. In tendering this homage to the Great Author of every public and private good, I assure myself that it expresses your sentiments not less than my own, nor those of my fellow citizens at large less than either. No people can be bound to acknowledge and adore the Invisible Hand which conducts the affairs of men more than those of the United States. Every step by which they have advanced to the character of an independent nation seems to have been distinguished by some token of providential agency; and in the important revolution just accomplished in the system of their united government the tranquil deliberations and voluntary consent of so many distinct communities from which the event has resulted can not be compared with the means by which most governments have been established without some return of pious gratitude, along with an humble anticipation of the future blessings which the past seem to presage. These reflections, arising out of the present crisis, have forced themselves too strongly on my mind to be suppressed. You will join with me, I trust, in thinking that there are none under the influence of which the proceedings of a new and free government can more auspiciously commence.\n","\n","By the article establishing the executive department it is made the duty of the President \"to recommend to your consideration such measures as he shall judge necessary and expedient.\" The circumstances under which I now meet you will acquit me from entering into that subject further than to refer to the great constitutional charter under which you are assembled, and which, in defining your powers, designates the objects to which your attention is to be given. It will be more consistent with those circumstances, and far more congenial with the feelings which actuate me, to substitute, in place of a recommendation of particular measures, the tribute that is due to the talents, the rectitude, and the patriotism which adorn the characters selected to devise and adopt them. In these honorable qualifications I behold the surest pledges that as on one side no local prejudices or attachments, no separate views nor party animosities, will misdirect the comprehensive and equal eye which ought to watch over this great assemblage of communities and interests, so, on another, that the foundation of our national policy will be laid in the pure and immutable principles of private morality, and the preeminence of free government be exemplified by all the attributes which can win the affections of its citizens and command the respect of the world. I dwell on this prospect with every satisfaction which an ardent love for my country can inspire, since there is no truth more thoroughly established than that there exists in the economy and course of nature an indissoluble union between virtue and happiness; between duty and advantage; between the genuine maxims of an honest and magnanimous policy and the solid rewards of public prosperity and felicity; since we ought to be no less persuaded that the propitious smiles of Heaven can never be expected on a nation that disregards the eternal rules of order and right which Heaven itself has ordained; and since the preservation of the sacred fire of liberty and the destiny of the republican model of government are justly considered, perhaps, as deeply, as finally, staked on the experiment entrusted to the hands of the American people.\n","\n","Besides the ordinary objects submitted to your care, it will remain with your judgment to decide how far an exercise of the occasional power delegated by the fifth article of the Constitution is rendered expedient at the present juncture by the nature of objections which have been urged against the system, or by the degree of inquietude which has given birth to them. Instead of undertaking particular recommendations on this subject, in which I could be guided by no lights derived from official opportunities, I shall again give way to my entire confidence in your discernment and pursuit of the public good; for I assure myself that whilst you carefully avoid every alteration which might endanger the benefits of an united and effective government, or which ought to await the future lessons of experience, a reverence for the characteristic rights of freemen and a regard for the public harmony will sufficiently influence your deliberations on the question how far the former can be impregnably fortified or the latter be safely and advantageously promoted.\n","\n","To the foregoing observations I have one to add, which will be most properly addressed to the House of Representatives. It concerns myself, and will therefore be as brief as possible. When I was first honored with a call into the service of my country, then on the eve of an arduous struggle for its liberties, the light in which I contemplated my duty required that I should renounce every pecuniary compensation. From this resolution I have in no instance departed; and being still under the impressions which produced it, I must decline as inapplicable to myself any share in the personal emoluments which may be indispensably included in a permanent provision for the executive department, and must accordingly pray that the pecuniary estimates for the station in which I am placed may during my continuance in it be limited to such actual expenditures as the public good may be thought to require.\n","\n","Having thus imparted to you my sentiments as they have been awakened by the occasion which brings us together, I shall take my present leave; but not without resorting once more to the benign Parent of the Human Race in humble supplication that, since He has been pleased to favor the American people with opportunities for deliberating in perfect tranquillity, and dispositions for deciding with unparalleled unanimity on a form of government for the security of their union and the advancement of their happiness, so His divine blessing may be equally conspicuous in the enlarged views, the temperate consultations, and the wise measures on which the success of this Government must depend. \n","\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package inaugural to /root/nltk_data...\n","[nltk_data]   Package inaugural is already up-to-date!\n"]},{"output_type":"display_data","data":{"text/plain":["     000   100   108    11   120   125    13  14th  15th    16  ...  yours  \\\n","0   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.02  0.00  0.00  ...   0.00   \n","1   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","2   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","3   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","4   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","5   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","6   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","7   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","8   0.03  0.00  0.00  0.00  0.01  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","9   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","10  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","11  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","12  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","13  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","14  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","15  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","16  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","17  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","18  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.01   \n","19  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","20  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","21  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","22  0.02  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","23  0.01  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","24  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.05   \n","25  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.01   \n","26  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","27  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.01  0.00  ...   0.00   \n","28  0.02  0.00  0.00  0.00  0.00  0.01  0.00  0.00  0.00  0.00  ...   0.00   \n","29  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","30  0.01  0.01  0.00  0.00  0.00  0.00  0.00  0.00  0.01  0.00  ...   0.00   \n","31  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","32  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","33  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","34  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","35  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","36  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.01   \n","37  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","38  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","39  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","40  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.02  ...   0.00   \n","41  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","42  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","43  0.02  0.02  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","44  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","45  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","46  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","47  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","48  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","49  0.00  0.00  0.00  0.00  0.00  0.00  0.02  0.00  0.00  0.00  ...   0.00   \n","50  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","51  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","52  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","53  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","54  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","55  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","56  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","57  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","58  0.01  0.00  0.02  0.02  0.00  0.00  0.00  0.00  0.00  0.00  ...   0.00   \n","\n","    yourself  yourselves  youth  youthful  youâ  zeal  zealous  zealously  \\\n","0       0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","1       0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","2       0.00        0.00   0.00      0.00  0.00  0.01     0.00       0.00   \n","3       0.00        0.00   0.00      0.00  0.00  0.01     0.00       0.00   \n","4       0.00        0.00   0.00      0.00  0.00  0.03     0.00       0.00   \n","5       0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.02   \n","6       0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","7       0.00        0.00   0.00      0.01  0.00  0.01     0.01       0.00   \n","8       0.00        0.00   0.00      0.00  0.00  0.00     0.01       0.01   \n","9       0.00        0.00   0.00      0.00  0.00  0.01     0.00       0.00   \n","10      0.00        0.00   0.00      0.00  0.00  0.02     0.02       0.00   \n","11      0.00        0.02   0.00      0.00  0.00  0.00     0.00       0.00   \n","12      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.01   \n","13      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","14      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","15      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.02   \n","16      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","17      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","18      0.00        0.01   0.00      0.00  0.00  0.00     0.00       0.00   \n","19      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","20      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","21      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","22      0.00        0.00   0.00      0.00  0.00  0.00     0.01       0.00   \n","23      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","24      0.00        0.00   0.00      0.00  0.00  0.01     0.00       0.00   \n","25      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","26      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","27      0.00        0.00   0.00      0.00  0.00  0.02     0.01       0.00   \n","28      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","29      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","30      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","31      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","32      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","33      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","34      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","35      0.00        0.00   0.01      0.00  0.00  0.00     0.00       0.01   \n","36      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","37      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","38      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","39      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","40      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","41      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","42      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","43      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","44      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","45      0.00        0.00   0.03      0.00  0.00  0.00     0.00       0.00   \n","46      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","47      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","48      0.00        0.01   0.00      0.00  0.00  0.00     0.00       0.00   \n","49      0.00        0.00   0.01      0.00  0.00  0.00     0.00       0.00   \n","50      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","51      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","52      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","53      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","54      0.02        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","55      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","56      0.00        0.00   0.01      0.00  0.00  0.00     0.00       0.00   \n","57      0.00        0.00   0.00      0.00  0.00  0.00     0.00       0.00   \n","58      0.00        0.00   0.00      0.00  0.02  0.00     0.00       0.00   \n","\n","    zone  \n","0   0.00  \n","1   0.00  \n","2   0.00  \n","3   0.00  \n","4   0.00  \n","5   0.00  \n","6   0.00  \n","7   0.01  \n","8   0.00  \n","9   0.00  \n","10  0.00  \n","11  0.00  \n","12  0.00  \n","13  0.00  \n","14  0.00  \n","15  0.00  \n","16  0.00  \n","17  0.00  \n","18  0.00  \n","19  0.00  \n","20  0.00  \n","21  0.00  \n","22  0.00  \n","23  0.00  \n","24  0.00  \n","25  0.00  \n","26  0.00  \n","27  0.00  \n","28  0.00  \n","29  0.00  \n","30  0.00  \n","31  0.00  \n","32  0.00  \n","33  0.00  \n","34  0.00  \n","35  0.00  \n","36  0.00  \n","37  0.00  \n","38  0.00  \n","39  0.00  \n","40  0.00  \n","41  0.00  \n","42  0.00  \n","43  0.00  \n","44  0.00  \n","45  0.00  \n","46  0.00  \n","47  0.00  \n","48  0.00  \n","49  0.00  \n","50  0.00  \n","51  0.00  \n","52  0.00  \n","53  0.00  \n","54  0.00  \n","55  0.00  \n","56  0.00  \n","57  0.00  \n","58  0.00  \n","\n","[59 rows x 9261 columns]"],"text/html":["\n","\n","  <div id=\"df-bbd44ca9-4ba0-44ce-9b9e-f03bc6562392\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>000</th>\n","      <th>100</th>\n","      <th>108</th>\n","      <th>11</th>\n","      <th>120</th>\n","      <th>125</th>\n","      <th>13</th>\n","      <th>14th</th>\n","      <th>15th</th>\n","      <th>16</th>\n","      <th>...</th>\n","      <th>yours</th>\n","      <th>yourself</th>\n","      <th>yourselves</th>\n","      <th>youth</th>\n","      <th>youthful</th>\n","      <th>youâ</th>\n","      <th>zeal</th>\n","      <th>zealous</th>\n","      <th>zealously</th>\n","      <th>zone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.03</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0.03</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.05</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>0.01</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>0.02</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.03</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>...</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59 rows × 9261 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbd44ca9-4ba0-44ce-9b9e-f03bc6562392')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","\n","\n","\n","    <div id=\"df-569ca82f-3565-44c7-a1da-ceb386523119\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-569ca82f-3565-44c7-a1da-ceb386523119')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","    </div>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","    <script>\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    </script>\n","\n","      <script>\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-569ca82f-3565-44c7-a1da-ceb386523119 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      </script>\n","      <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bbd44ca9-4ba0-44ce-9b9e-f03bc6562392 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bbd44ca9-4ba0-44ce-9b9e-f03bc6562392');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"6Bckx5RzyUob","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689811405395,"user_tz":240,"elapsed":1591,"user":{"displayName":"Lakshmikar Reddy Polamreddy","userId":"01522637219110771790"}},"outputId":"b28f6b97-7b48-4767-eccd-d8b63eba6d0c"},"source":["### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n","import math\n","import pandas as pd\n","from collections import Counter\n","from nltk.corpus import inaugural\n","\n","# Step 1: Download the inaugural dataset from NLTK\n","nltk.download('inaugural')\n","\n","# Step 2: Load the dataset and preprocess the text data\n","def load_inaugural_speeches():\n","    speeches = [inaugural.raw(fileid) for fileid in inaugural.fileids()]\n","    return speeches\n","\n","speeches = load_inaugural_speeches()\n","\n","# Step 3: Calculate the Term Frequency (TF) for each word in each document\n","def calculate_tf(document):\n","    word_frequency = Counter(document.split())\n","    total_words = len(document.split())\n","    tf_values = {word: freq / total_words for word, freq in word_frequency.items()}\n","    return tf_values\n","\n","tf_values_per_document = [calculate_tf(speech) for speech in speeches]\n","\n","# Step 4: Calculate the Document Frequency (DF) for each word in the entire corpus\n","def calculate_df(corpus):\n","    df_values = Counter()\n","    for document in corpus:\n","        words = set(document.split())\n","        df_values.update(words)\n","    return df_values\n","\n","df_values_corpus = calculate_df(speeches)\n","\n","# Step 5: Calculate the Inverse Document Frequency (IDF) for each word\n","def calculate_idf(corpus_size, df_values):\n","    idf_values = {word: math.log(corpus_size / (df + 1)) for word, df in df_values.items()}\n","    return idf_values\n","\n","corpus_size = len(speeches)\n","idf_values = calculate_idf(corpus_size, df_values_corpus)\n","\n","# Step 6: Calculate the TF-IDF for each word in each document\n","def calculate_tfidf(tf_values, idf_values):\n","    tfidf_values = {word: tf_values[word] * idf_values[word] for word in tf_values.keys()}\n","    return tfidf_values\n","\n","tfidf_values_per_document = [calculate_tfidf(tf_values, idf_values) for tf_values in tf_values_per_document]\n","\n","# Convert the list of dictionaries into a DataFrame\n","tfidf_df = pd.DataFrame(tfidf_values_per_document)\n","\n","# Fill NaN values with 0 (for words that are not present in a specific document)\n","tfidf_df = tfidf_df.fillna(0)\n","\n","# Add a column to represent the speech/document index\n","tfidf_df['Speech'] = inaugural.fileids()\n","\n","# Set the 'Speech' column as the index\n","tfidf_df.set_index('Speech', inplace=True)\n","\n","print(tfidf_df)\n","\n","### END CODE HERE ###"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package inaugural to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/inaugural.zip.\n"]},{"output_type":"stream","name":"stdout","text":["                     Fellow-Citizens        of       the    Senate       and  \\\n","Speech                                                                         \n","1789-Washington.txt         0.002082 -0.000834 -0.001351  0.001490 -0.000552   \n","1793-Washington.txt         0.000000 -0.001369 -0.001618  0.000000 -0.000249   \n","1797-Adams.txt              0.000000 -0.001015 -0.001145  0.000919 -0.000913   \n","1801-Jefferson.txt          0.000000 -0.001010 -0.001244  0.000000 -0.000767   \n","1805-Jefferson.txt          0.000000 -0.000783 -0.001070  0.000000 -0.000721   \n","1809-Madison.txt            0.000000 -0.000971 -0.001457  0.000000 -0.000614   \n","1813-Madison.txt            0.000000 -0.000903 -0.001320  0.000000 -0.000569   \n","1817-Monroe.txt             0.000000 -0.000807 -0.001315  0.000000 -0.000583   \n","1821-Monroe.txt             0.000000 -0.000737 -0.001260  0.000000 -0.000511   \n","1825-Adams.txt              0.000000 -0.001404 -0.001652  0.000000 -0.000662   \n","1829-Jackson.txt            0.000000 -0.001056 -0.001309  0.000000 -0.000684   \n","1833-Jackson.txt            0.000000 -0.001084 -0.001370  0.000000 -0.000728   \n","1837-VanBuren.txt           0.000000 -0.000865 -0.001049  0.000000 -0.000647   \n","1841-Harrison.txt           0.000000 -0.001193 -0.001579  0.000000 -0.000441   \n","1845-Polk.txt               0.000000 -0.001042 -0.001329  0.000000 -0.000654   \n","1849-Taylor.txt             0.000000 -0.000957 -0.001466  0.000000 -0.000772   \n","1853-Pierce.txt             0.000000 -0.000841 -0.001062  0.000000 -0.000624   \n","1857-Buchanan.txt           0.000000 -0.000827 -0.001368  0.000000 -0.000577   \n","1861-Lincoln.txt            0.000819 -0.000674 -0.001104  0.000000 -0.000462   \n","1865-Lincoln.txt            0.000000 -0.000530 -0.001300  0.000000 -0.000578   \n","1869-Grant.txt              0.000000 -0.000700 -0.001117  0.000000 -0.000402   \n","1873-Grant.txt              0.000000 -0.000902 -0.001253  0.000000 -0.000589   \n","1877-Hayes.txt              0.000000 -0.001121 -0.001526  0.000000 -0.000669   \n","1881-Garfield.txt           0.000000 -0.001022 -0.001631  0.000000 -0.000649   \n","1885-Cleveland.txt          0.000000 -0.001167 -0.001646  0.000000 -0.001007   \n","1889-Harrison.txt           0.000000 -0.000919 -0.001199  0.000000 -0.000720   \n","1893-Cleveland.txt          0.000000 -0.000994 -0.001211  0.000000 -0.000835   \n","1897-McKinley.txt           0.000000 -0.000965 -0.001316  0.001610 -0.000719   \n","1901-McKinley.txt           0.000000 -0.000836 -0.001375  0.000000 -0.000729   \n","1905-Roosevelt.txt          0.000000 -0.000769 -0.001093  0.000000 -0.000649   \n","1909-Taft.txt               0.000000 -0.000969 -0.001368  0.000000 -0.000675   \n","1913-Wilson.txt             0.000000 -0.000861 -0.000979  0.001255 -0.000762   \n","1917-Wilson.txt             0.000000 -0.000833 -0.000965  0.000000 -0.000811   \n","1921-Harding.txt            0.000000 -0.000803 -0.000944  0.000000 -0.000767   \n","1925-Coolidge.txt           0.000000 -0.000858 -0.000970  0.000000 -0.000601   \n","1929-Hoover.txt             0.000000 -0.001151 -0.001217  0.000000 -0.000571   \n","1933-Roosevelt.txt          0.000000 -0.000972 -0.001106  0.000000 -0.000499   \n","1937-Roosevelt.txt          0.000000 -0.000981 -0.000935  0.000000 -0.000481   \n","1941-Roosevelt.txt          0.000000 -0.001001 -0.001285  0.000000 -0.000507   \n","1945-Roosevelt.txt          0.000000 -0.000736 -0.000706  0.000000 -0.000559   \n","1949-Truman.txt             0.000000 -0.000708 -0.000951  0.000935 -0.000723   \n","1953-Eisenhower.txt         0.000000 -0.000968 -0.001111  0.000000 -0.000661   \n","1957-Eisenhower.txt         0.000000 -0.000959 -0.001059  0.000000 -0.000489   \n","1961-Kennedy.txt            0.000000 -0.000786 -0.001004  0.000000 -0.000447   \n","1965-Johnson.txt            0.000000 -0.000639 -0.000807  0.000000 -0.000672   \n","1969-Nixon.txt              0.000000 -0.000738 -0.000981  0.000000 -0.000290   \n","1973-Nixon.txt              0.000000 -0.000628 -0.000739  0.000000 -0.000434   \n","1977-Carter.txt             0.000000 -0.000450 -0.000668  0.000000 -0.000613   \n","1981-Reagan.txt             0.000000 -0.000613 -0.000785  0.000000 -0.000585   \n","1985-Reagan.txt             0.000000 -0.000620 -0.000842  0.000000 -0.000639   \n","1989-Bush.txt               0.000000 -0.000441 -0.000803  0.000000 -0.000600   \n","1993-Clinton.txt            0.000000 -0.000484 -0.000894  0.000000 -0.000663   \n","1997-Clinton.txt            0.000000 -0.000744 -0.000953  0.000000 -0.000612   \n","2001-Bush.txt               0.000000 -0.000616 -0.000510  0.000000 -0.000680   \n","2005-Bush.txt               0.000000 -0.000948 -0.001087  0.000000 -0.000777   \n","2009-Obama.txt              0.000000 -0.000571 -0.000877  0.000000 -0.000731   \n","2013-Obama.txt              0.000000 -0.000555 -0.000804  0.000000 -0.000676   \n","2017-Trump.txt              0.000000 -0.000554 -0.000750  0.000000 -0.000808   \n","2021-Biden.txt              0.000000 -0.000544 -0.000716  0.000841 -0.000524   \n","\n","                        House  Representatives:     Among  vicissitudes  \\\n","Speech                                                                    \n","1789-Washington.txt  0.002979          0.002365  0.001725      0.001881   \n","1793-Washington.txt  0.000000          0.000000  0.000000      0.000000   \n","1797-Adams.txt       0.000000          0.000000  0.000000      0.000000   \n","1801-Jefferson.txt   0.000000          0.000000  0.000000      0.000000   \n","1805-Jefferson.txt   0.000000          0.000000  0.000000      0.000000   \n","1809-Madison.txt     0.000000          0.000000  0.000000      0.000000   \n","1813-Madison.txt     0.000000          0.000000  0.000000      0.000000   \n","1817-Monroe.txt      0.000000          0.000000  0.000000      0.000000   \n","1821-Monroe.txt      0.000000          0.000000  0.000000      0.000000   \n","1825-Adams.txt       0.000000          0.000000  0.000000      0.000922   \n","1829-Jackson.txt     0.000000          0.000000  0.000000      0.000000   \n","1833-Jackson.txt     0.000000          0.000000  0.000000      0.000000   \n","1837-VanBuren.txt    0.000000          0.000000  0.000642      0.000700   \n","1841-Harrison.txt    0.000253          0.000000  0.000000      0.000000   \n","1845-Polk.txt        0.000000          0.000000  0.000000      0.000000   \n","1849-Taylor.txt      0.000000          0.000000  0.000000      0.000000   \n","1853-Pierce.txt      0.000000          0.000000  0.000000      0.000000   \n","1857-Buchanan.txt    0.000000          0.000000  0.000874      0.000000   \n","1861-Lincoln.txt     0.000000          0.000000  0.000000      0.000000   \n","1865-Lincoln.txt     0.000000          0.000000  0.000000      0.000000   \n","1869-Grant.txt       0.000000          0.000000  0.000000      0.000000   \n","1873-Grant.txt       0.000000          0.000000  0.000000      0.000000   \n","1877-Hayes.txt       0.000000          0.000000  0.000000      0.000000   \n","1881-Garfield.txt    0.000000          0.000000  0.000000      0.000000   \n","1885-Cleveland.txt   0.000000          0.000000  0.000000      0.000000   \n","1889-Harrison.txt    0.000000          0.000000  0.000000      0.000000   \n","1893-Cleveland.txt   0.000000          0.000000  0.000000      0.000000   \n","1897-McKinley.txt    0.001073          0.000000  0.000000      0.000000   \n","1901-McKinley.txt    0.000000          0.000000  0.000000      0.000000   \n","1905-Roosevelt.txt   0.000000          0.000000  0.000000      0.000000   \n","1909-Taft.txt        0.000000          0.000000  0.000000      0.000000   \n","1913-Wilson.txt      0.001255          0.000000  0.000000      0.000000   \n","1917-Wilson.txt      0.000000          0.000000  0.000000      0.000000   \n","1921-Harding.txt     0.000000          0.000000  0.000000      0.000000   \n","1925-Coolidge.txt    0.000000          0.000000  0.000000      0.000000   \n","1929-Hoover.txt      0.000000          0.000000  0.000000      0.000000   \n","1933-Roosevelt.txt   0.000000          0.000000  0.000000      0.000000   \n","1937-Roosevelt.txt   0.000000          0.000000  0.001359      0.000000   \n","1941-Roosevelt.txt   0.000000          0.000000  0.000000      0.000000   \n","1945-Roosevelt.txt   0.000000          0.000000  0.000000      0.000000   \n","1949-Truman.txt      0.000000          0.000000  0.000000      0.000000   \n","1953-Eisenhower.txt  0.000000          0.000000  0.000000      0.000000   \n","1957-Eisenhower.txt  0.000000          0.000000  0.000000      0.000000   \n","1961-Kennedy.txt     0.000000          0.000000  0.000000      0.000000   \n","1965-Johnson.txt     0.000000          0.000000  0.000000      0.000000   \n","1969-Nixon.txt       0.000000          0.000000  0.000000      0.000000   \n","1973-Nixon.txt       0.000000          0.000000  0.000000      0.000000   \n","1977-Carter.txt      0.000000          0.000000  0.000000      0.000000   \n","1981-Reagan.txt      0.000000          0.000000  0.000000      0.000000   \n","1985-Reagan.txt      0.000000          0.000000  0.000000      0.000000   \n","1989-Bush.txt        0.000917          0.000000  0.000000      0.000000   \n","1993-Clinton.txt     0.000000          0.000000  0.000000      0.000000   \n","1997-Clinton.txt     0.000000          0.000000  0.000000      0.000000   \n","2001-Bush.txt        0.000000          0.000000  0.000000      0.000000   \n","2005-Bush.txt        0.000000          0.000000  0.000000      0.000000   \n","2009-Obama.txt       0.000000          0.000000  0.000000      0.000000   \n","2013-Obama.txt       0.000000          0.000000  0.000000      0.000000   \n","2017-Trump.txt       0.000000          0.000000  0.000000      0.000000   \n","2021-Biden.txt       0.000841          0.000000  0.000000      0.000000   \n","\n","                     incident  ...   youâI  possibilities;     fear;  \\\n","Speech                         ...                                       \n","1789-Washington.txt  0.001490  ...  0.000000        0.000000  0.000000   \n","1793-Washington.txt  0.000000  ...  0.000000        0.000000  0.000000   \n","1797-Adams.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","1801-Jefferson.txt   0.000000  ...  0.000000        0.000000  0.000000   \n","1805-Jefferson.txt   0.000000  ...  0.000000        0.000000  0.000000   \n","1809-Madison.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1813-Madison.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1817-Monroe.txt      0.001264  ...  0.000000        0.000000  0.000000   \n","1821-Monroe.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1825-Adams.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","1829-Jackson.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1833-Jackson.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1837-VanBuren.txt    0.000000  ...  0.000000        0.000000  0.000000   \n","1841-Harrison.txt    0.000000  ...  0.000000        0.000000  0.000000   \n","1845-Polk.txt        0.000444  ...  0.000000        0.000000  0.000000   \n","1849-Taylor.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1853-Pierce.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1857-Buchanan.txt    0.000000  ...  0.000000        0.000000  0.000000   \n","1861-Lincoln.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1865-Lincoln.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1869-Grant.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","1873-Grant.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","1877-Hayes.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","1881-Garfield.txt    0.000000  ...  0.000000        0.000000  0.000000   \n","1885-Cleveland.txt   0.000000  ...  0.000000        0.000000  0.000000   \n","1889-Harrison.txt    0.000486  ...  0.000000        0.000000  0.000000   \n","1893-Cleveland.txt   0.001059  ...  0.000000        0.000000  0.000000   \n","1897-McKinley.txt    0.000000  ...  0.000000        0.000000  0.000000   \n","1901-McKinley.txt    0.000000  ...  0.000000        0.000000  0.000000   \n","1905-Roosevelt.txt   0.000000  ...  0.000000        0.000000  0.000000   \n","1909-Taft.txt        0.000000  ...  0.000000        0.000000  0.000000   \n","1913-Wilson.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1917-Wilson.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1921-Harding.txt     0.000640  ...  0.000000        0.000000  0.000000   \n","1925-Coolidge.txt    0.000000  ...  0.000000        0.000000  0.000000   \n","1929-Hoover.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1933-Roosevelt.txt   0.000000  ...  0.000000        0.000000  0.000000   \n","1937-Roosevelt.txt   0.000000  ...  0.000000        0.000000  0.000000   \n","1941-Roosevelt.txt   0.000000  ...  0.000000        0.000000  0.000000   \n","1945-Roosevelt.txt   0.000000  ...  0.000000        0.000000  0.000000   \n","1949-Truman.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1953-Eisenhower.txt  0.000000  ...  0.000000        0.000000  0.000000   \n","1957-Eisenhower.txt  0.000000  ...  0.000000        0.000000  0.000000   \n","1961-Kennedy.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1965-Johnson.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1969-Nixon.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","1973-Nixon.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","1977-Carter.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1981-Reagan.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1985-Reagan.txt      0.000000  ...  0.000000        0.000000  0.000000   \n","1989-Bush.txt        0.000000  ...  0.000000        0.000000  0.000000   \n","1993-Clinton.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","1997-Clinton.txt     0.000000  ...  0.000000        0.000000  0.000000   \n","2001-Bush.txt        0.000000  ...  0.000000        0.000000  0.000000   \n","2005-Bush.txt        0.000000  ...  0.000000        0.000000  0.000000   \n","2009-Obama.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","2013-Obama.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","2017-Trump.txt       0.000000  ...  0.000000        0.000000  0.000000   \n","2021-Biden.txt       0.000000  ...  0.001335        0.001335  0.001335   \n","\n","                     division;  healing,   moment;    watch,  thrived;  \\\n","Speech                                                                   \n","1789-Washington.txt   0.000000  0.000000  0.000000  0.000000  0.000000   \n","1793-Washington.txt   0.000000  0.000000  0.000000  0.000000  0.000000   \n","1797-Adams.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","1801-Jefferson.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1805-Jefferson.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1809-Madison.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1813-Madison.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1817-Monroe.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1821-Monroe.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1825-Adams.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","1829-Jackson.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1833-Jackson.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1837-VanBuren.txt     0.000000  0.000000  0.000000  0.000000  0.000000   \n","1841-Harrison.txt     0.000000  0.000000  0.000000  0.000000  0.000000   \n","1845-Polk.txt         0.000000  0.000000  0.000000  0.000000  0.000000   \n","1849-Taylor.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1853-Pierce.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1857-Buchanan.txt     0.000000  0.000000  0.000000  0.000000  0.000000   \n","1861-Lincoln.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1865-Lincoln.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1869-Grant.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","1873-Grant.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","1877-Hayes.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","1881-Garfield.txt     0.000000  0.000000  0.000000  0.000000  0.000000   \n","1885-Cleveland.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1889-Harrison.txt     0.000000  0.000000  0.000000  0.000000  0.000000   \n","1893-Cleveland.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1897-McKinley.txt     0.000000  0.000000  0.000000  0.000000  0.000000   \n","1901-McKinley.txt     0.000000  0.000000  0.000000  0.000000  0.000000   \n","1905-Roosevelt.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1909-Taft.txt         0.000000  0.000000  0.000000  0.000000  0.000000   \n","1913-Wilson.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1917-Wilson.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1921-Harding.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1925-Coolidge.txt     0.000000  0.000000  0.000000  0.000000  0.000000   \n","1929-Hoover.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1933-Roosevelt.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1937-Roosevelt.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1941-Roosevelt.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1945-Roosevelt.txt    0.000000  0.000000  0.000000  0.000000  0.000000   \n","1949-Truman.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1953-Eisenhower.txt   0.000000  0.000000  0.000000  0.000000  0.000000   \n","1957-Eisenhower.txt   0.000000  0.000000  0.000000  0.000000  0.000000   \n","1961-Kennedy.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1965-Johnson.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1969-Nixon.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","1973-Nixon.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","1977-Carter.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1981-Reagan.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1985-Reagan.txt       0.000000  0.000000  0.000000  0.000000  0.000000   \n","1989-Bush.txt         0.000000  0.000000  0.000000  0.000000  0.000000   \n","1993-Clinton.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","1997-Clinton.txt      0.000000  0.000000  0.000000  0.000000  0.000000   \n","2001-Bush.txt         0.000000  0.000000  0.000000  0.000000  0.000000   \n","2005-Bush.txt         0.000000  0.000000  0.000000  0.000000  0.000000   \n","2009-Obama.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","2013-Obama.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","2017-Trump.txt        0.000000  0.000000  0.000000  0.000000  0.000000   \n","2021-Biden.txt        0.001335  0.001335  0.001335  0.001335  0.001335   \n","\n","                     forebearers,   troops.  \n","Speech                                       \n","1789-Washington.txt      0.000000  0.000000  \n","1793-Washington.txt      0.000000  0.000000  \n","1797-Adams.txt           0.000000  0.000000  \n","1801-Jefferson.txt       0.000000  0.000000  \n","1805-Jefferson.txt       0.000000  0.000000  \n","1809-Madison.txt         0.000000  0.000000  \n","1813-Madison.txt         0.000000  0.000000  \n","1817-Monroe.txt          0.000000  0.000000  \n","1821-Monroe.txt          0.000000  0.000000  \n","1825-Adams.txt           0.000000  0.000000  \n","1829-Jackson.txt         0.000000  0.000000  \n","1833-Jackson.txt         0.000000  0.000000  \n","1837-VanBuren.txt        0.000000  0.000000  \n","1841-Harrison.txt        0.000000  0.000000  \n","1845-Polk.txt            0.000000  0.000000  \n","1849-Taylor.txt          0.000000  0.000000  \n","1853-Pierce.txt          0.000000  0.000000  \n","1857-Buchanan.txt        0.000000  0.000000  \n","1861-Lincoln.txt         0.000000  0.000000  \n","1865-Lincoln.txt         0.000000  0.000000  \n","1869-Grant.txt           0.000000  0.000000  \n","1873-Grant.txt           0.000000  0.000000  \n","1877-Hayes.txt           0.000000  0.000000  \n","1881-Garfield.txt        0.000000  0.000000  \n","1885-Cleveland.txt       0.000000  0.000000  \n","1889-Harrison.txt        0.000000  0.000000  \n","1893-Cleveland.txt       0.000000  0.000000  \n","1897-McKinley.txt        0.000000  0.000000  \n","1901-McKinley.txt        0.000000  0.000000  \n","1905-Roosevelt.txt       0.000000  0.000000  \n","1909-Taft.txt            0.000000  0.000000  \n","1913-Wilson.txt          0.000000  0.000000  \n","1917-Wilson.txt          0.000000  0.000000  \n","1921-Harding.txt         0.000000  0.000000  \n","1925-Coolidge.txt        0.000000  0.000000  \n","1929-Hoover.txt          0.000000  0.000000  \n","1933-Roosevelt.txt       0.000000  0.000000  \n","1937-Roosevelt.txt       0.000000  0.000000  \n","1941-Roosevelt.txt       0.000000  0.000000  \n","1945-Roosevelt.txt       0.000000  0.000000  \n","1949-Truman.txt          0.000000  0.000000  \n","1953-Eisenhower.txt      0.000000  0.000000  \n","1957-Eisenhower.txt      0.000000  0.000000  \n","1961-Kennedy.txt         0.000000  0.000000  \n","1965-Johnson.txt         0.000000  0.000000  \n","1969-Nixon.txt           0.000000  0.000000  \n","1973-Nixon.txt           0.000000  0.000000  \n","1977-Carter.txt          0.000000  0.000000  \n","1981-Reagan.txt          0.000000  0.000000  \n","1985-Reagan.txt          0.000000  0.000000  \n","1989-Bush.txt            0.000000  0.000000  \n","1993-Clinton.txt         0.000000  0.000000  \n","1997-Clinton.txt         0.000000  0.000000  \n","2001-Bush.txt            0.000000  0.000000  \n","2005-Bush.txt            0.000000  0.000000  \n","2009-Obama.txt           0.000000  0.000000  \n","2013-Obama.txt           0.000000  0.000000  \n","2017-Trump.txt           0.000000  0.000000  \n","2021-Biden.txt           0.001335  0.001335  \n","\n","[59 rows x 14993 columns]\n"]}]},{"cell_type":"markdown","metadata":{"id":"QEGeTp9idVUk"},"source":["##A. References\n","\n","1.   Chapter 6 – Vector Semantics and Word Embeddings Speech and Language rocessing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021.\n","2.   [Word2vec from Scratch with NumPy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)\n","3.   [A hands=on intutive approach to Deep Learning Methods for Text Data - Word2Vec,GloVe and FastText](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n","4.    [Traditional Methods for Text Data](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)\n","5.    [Word Embeddings](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb#scrollTo=Q6mJg1g3apaz)\n","6. [CS 224D: Deep Learning for NLP](https://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf)\n","7. [Text Vectorization] (https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html)\n","8. [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)\n","9. [TF-IDF](https://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)\n","10. [Applying TF-IDF algorithm in practice](https://plumbr.io/blog/programming/applying-tf-idf-algorithm-in-practice)\n","11. [text2vec](http://text2vec.org/similarity.html)\n","\n"]}]}